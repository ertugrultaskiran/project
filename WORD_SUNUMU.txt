â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÄ°TÄ°RME PROJESÄ°
MÃ¼ÅŸteri Destek Taleplerinin Otomatik SÄ±nÄ±flandÄ±rÄ±lmasÄ±
NLP ve Derin Ã–ÄŸrenme ile Topic Modelling
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã–ÄŸrenci: [AdÄ±nÄ±z SoyadÄ±nÄ±z]
DanÄ±ÅŸman: [Hoca AdÄ±]
Tarih: 21 Ekim 2025

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


1. PROJE AMACI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Problem:
â€¢ Åirketlerde gÃ¼nde yÃ¼zlerce IT destek talebi geliyor
â€¢ Taleplerin doÄŸru departmana yÃ¶nlendirilmesi zaman alÄ±yor
â€¢ Manuel iÅŸlem hatalara ve gecikmelere yol aÃ§Ä±yor

Ã‡Ã¶zÃ¼m:
â€¢ Yapay zeka ile otomatik sÄ±nÄ±flandÄ±rma sistemi
â€¢ Gelen talepleri anlÄ±k olarak doÄŸru kategoriye atama
â€¢ %88.82 doÄŸruluk oranÄ± ile Ã§alÄ±ÅŸan akÄ±llÄ± sistem


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


2. VERÄ° SETÄ°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Veri KaynaÄŸÄ±:
â€¢ IT Destek Sistemi kayÄ±tlarÄ±
â€¢ GerÃ§ek ÅŸirket verileri
â€¢ Ä°ngilizce metin verileri

Veri Boyutu:
â€¢ Toplam: 47,837 destek talebi (ticket)
â€¢ SÃ¼tunlar: text (talep metni), label (kategori)
â€¢ 8 farklÄ± kategori

Kategoriler ve DaÄŸÄ±lÄ±m:
1. Hardware (DonanÄ±m)                13,617 ticket (%28.5)
2. HR Support (Ä°nsan KaynaklarÄ±)     10,915 ticket (%22.8)
3. Access (EriÅŸim Ä°zinleri)           7,125 ticket (%14.9)
4. Miscellaneous (DiÄŸer)              7,060 ticket (%14.8)
5. Storage (Depolama)                 2,777 ticket (%5.8)
6. Purchase (SatÄ±n Alma)              2,464 ticket (%5.2)
7. Internal Project (Ä°Ã§ Projeler)     2,119 ticket (%4.4)
8. Administrative Rights (Admin)      1,760 ticket (%3.7)

Veri BÃ¶lÃ¼mleme:
â€¢ Train Set: %80 (38,269 ticket) - Model eÄŸitimi
â€¢ Validation Set: %10 (4,784 ticket) - Parametre ayarÄ±
â€¢ Test Set: %10 (4,784 ticket) - Final deÄŸerlendirme


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


3. KULLANILAN YÃ–NTEMLER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Projem 4 farklÄ± makine Ã¶ÄŸrenmesi ve derin Ã¶ÄŸrenme yaklaÅŸÄ±mÄ±nÄ± 
karÅŸÄ±laÅŸtÄ±rmalÄ± olarak iÃ§ermektedir:


MODEL 1: BASELINE - TF-IDF + Logistic Regression
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AÃ§Ä±klama:
Klasik makine Ã¶ÄŸrenmesi yaklaÅŸÄ±mÄ±. Metin verilerini TF-IDF ile 
sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼p Logistic Regression ile sÄ±nÄ±flandÄ±rma.

Teknik Detay:
â€¢ TF-IDF Vectorization
  - min_df: 3 (En az 3 dokÃ¼manda geÃ§en kelimeler)
  - max_df: 0.9 (Ã‡ok sÄ±k geÃ§enleri filtrele)
  - ngram_range: (1,2) (Tek ve iki kelime kombinasyonlarÄ±)

â€¢ Logistic Regression
  - max_iter: 200
  - Class weights: Dengesiz sÄ±nÄ±f problemi Ã§Ã¶zÃ¼mÃ¼

Avantajlar:
âœ“ Ã‡ok hÄ±zlÄ± eÄŸitim (5 dakika)
âœ“ AÃ§Ä±klanabilir model
âœ“ Az hesaplama kaynaÄŸÄ±
âœ“ Stabil ve gÃ¼venilir

Dezavantajlar:
âœ— Kelime sÄ±rasÄ±nÄ± anlamaz
âœ— BaÄŸlamsal iliÅŸkileri kaÃ§Ä±rÄ±r

SonuÃ§: %86.04 Test Accuracy


MODEL 2: DEEP LEARNING - Word2Vec + Bidirectional LSTM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AÃ§Ä±klama:
Derin Ã¶ÄŸrenme yaklaÅŸÄ±mÄ±. Word2Vec ile anlamsal kelime vektÃ¶rleri 
oluÅŸturup, Bidirectional LSTM ile sÄ±ralÄ± metin analizi.

Teknik Detay:
â€¢ Word2Vec Embedding
  - Vector size: 200 boyut
  - Window: 5 (BaÄŸlam penceresi)
  - Skip-gram algoritmasÄ±
  - Vocabulary: 11,661 kelime

â€¢ Bidirectional LSTM
  - 128 LSTM units
  - Dropout: 0.3 (Overfitting Ã¶nleme)
  - GlobalMaxPooling
  - 8 sÄ±nÄ±f iÃ§in softmax Ã§Ä±kÄ±ÅŸ

Mimari:
Input â†’ Embedding (200d) â†’ SpatialDropout â†’ BiLSTM (128) â†’ 
GlobalMaxPooling â†’ Dropout â†’ Dense (8) â†’ Softmax

Avantajlar:
âœ“ Kelime sÄ±rasÄ±nÄ± anlar
âœ“ BaÄŸlamsal iliÅŸkileri yakalar
âœ“ Anlamsal benzerlik (car â‰ˆ automobile)
âœ“ Ä°ki yÃ¶nlÃ¼ okuma (ileri-geri)

Dezavantajlar:
âœ— YavaÅŸ eÄŸitim (30 dakika)
âœ— GPU tercih edilir
âœ— Daha karmaÅŸÄ±k

SonuÃ§: %87.00 Test Accuracy (+0.96%)


MODEL 3: ENSEMBLE - Baseline + LSTM Kombinasyonu
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AÃ§Ä±klama:
Birden fazla modelin tahminlerini birleÅŸtirerek daha gÃ¼venilir 
sonuÃ§lar elde etme yaklaÅŸÄ±mÄ±.

Teknik Detay:
â€¢ Weighted Average Ensemble
  - Baseline weight: 0.5
  - LSTM weight: 0.5
  - FarklÄ± kombinasyonlar test edildi
  - En iyi: Equal weights

YÃ¶ntem:
Ensemble = (0.5 Ã— Baseline_Prob) + (0.5 Ã— LSTM_Prob)

Avantajlar:
âœ“ Daha yÃ¼ksek doÄŸruluk
âœ“ Daha gÃ¼venilir tahminler
âœ“ Model hatalarÄ±nÄ± dengeler
âœ“ Kolay implementasyon

Dezavantajlar:
âœ— Ä°ki model birlikte Ã§alÄ±ÅŸmalÄ±
âœ— Tahmin sÃ¼resi 2 katÄ±na Ã§Ä±kar
âœ— Deployment karmaÅŸÄ±k

SonuÃ§: %88.40 Test Accuracy (+2.36%)


MODEL 4: BERT - Transfer Learning (Fine-Tuning)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AÃ§Ä±klama:
Google tarafÄ±ndan geliÅŸtirilmiÅŸ, milyarlarca kelime ile Ã¶nceden 
eÄŸitilmiÅŸ transformer modelinin bizim veriye uyarlanmasÄ±.

Teknik Detay:
â€¢ Pre-trained Model: bert-base-uncased
  - 110 milyon parametre
  - 12 transformer layer
  - 768 boyutlu hidden states

â€¢ Fine-tuning Parametreleri:
  - Epochs: 3
  - Batch size: 16
  - Learning rate: 2e-5
  - Max length: 128 token
  - Optimizer: AdamW

â€¢ Hardware:
  - GPU: NVIDIA GeForce RTX 2060
  - CUDA acceleration
  - EÄŸitim sÃ¼resi: ~2.5 saat

Attention Mechanism:
Her kelimenin diÄŸer tÃ¼m kelimelerle iliÅŸkisini analiz eder.
BaÄŸlamÄ± tam olarak anlar.

Avantajlar:
âœ“ En yÃ¼ksek performans (%88.82)
âœ“ Transfer learning (milyarlarca kelime bilgisi)
âœ“ Attention mechanism (derin baÄŸlam anlama)
âœ“ Robust (farklÄ± yazÄ±m stillerine dayanÄ±klÄ±)
âœ“ State-of-the-art teknoloji

Dezavantajlar:
âœ— YavaÅŸ eÄŸitim (~2.5 saat)
âœ— GPU zorunlu
âœ— BÃ¼yÃ¼k model boyutu (~400 MB)
âœ— YÃ¼ksek kaynak tÃ¼ketimi

SonuÃ§: %88.82 Test Accuracy (+2.78%) ğŸ† EN Ä°YÄ° MODEL


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


4. SONUÃ‡LAR VE KARÅILAÅTIRMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model                       â”‚ Accuracy â”‚ EÄŸitim     â”‚ Tahmin      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Baseline (TF-IDF + LogReg)  â”‚  86.04%  â”‚   5 dk     â”‚  0.01 sn    â”‚
â”‚ LSTM (Word2Vec + BiLSTM)    â”‚  87.00%  â”‚  30 dk     â”‚  0.05 sn    â”‚
â”‚ Ensemble (Base + LSTM)      â”‚  88.40%  â”‚   0 dk     â”‚  0.06 sn    â”‚
â”‚ BERT Fine-tuned             â”‚  88.82%  â”‚ 150 dk     â”‚  0.10 sn    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ä°yileÅŸtirme OranlarÄ±:
â€¢ LSTM vs Baseline:    +0.96%
â€¢ Ensemble vs Baseline: +2.36%
â€¢ BERT vs Baseline:     +2.78%


DetaylÄ± Metrikler (BERT - En Ä°yi Model):

SÄ±nÄ±f                    Precision   Recall   F1-Score   Support
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purchase                   0.96      0.91      0.94        247
Storage                    0.93      0.91      0.92        277
Access                     0.90      0.93      0.92        713
HR Support                 0.91      0.89      0.90      1,091
Internal Project           0.91      0.87      0.89        212
Hardware                   0.87      0.88      0.88      1,362
Miscellaneous              0.84      0.85      0.85        706
Administrative rights      0.88      0.81      0.84        176
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Accuracy                                       0.89      4,784
Macro avg                  0.90      0.88      0.89      4,784
Weighted avg               0.89      0.89      0.89      4,784


En BaÅŸarÄ±lÄ± SÄ±nÄ±flar:
1. Purchase:         F1 = 0.938 (MÃ¼kemmel!)
2. Storage:          F1 = 0.922 (Ã‡ok Ä°yi!)
3. Access:           F1 = 0.915 (Harika!)

GeliÅŸtirilebilir SÄ±nÄ±flar:
1. Administrative rights: F1 = 0.840 (Az veri)
2. Miscellaneous:         F1 = 0.847 (KarÄ±ÅŸÄ±k kategori)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


5. KULLANILAN TEKNOLOJÄ°LER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Programlama Dili:
â€¢ Python 3.13

Makine Ã–ÄŸrenmesi KÃ¼tÃ¼phaneleri:
â€¢ Scikit-learn (Baseline model)
â€¢ TensorFlow 2.20 (LSTM model)
â€¢ PyTorch 2.9 (BERT model)
â€¢ Transformers 4.57 (Hugging Face BERT)
â€¢ Gensim 4.0+ (Word2Vec)

Veri Ä°ÅŸleme:
â€¢ Pandas (Veri manipÃ¼lasyonu)
â€¢ NumPy (SayÄ±sal iÅŸlemler)
â€¢ NLTK / Regex (Metin temizleme)

GÃ¶rselleÅŸtirme:
â€¢ Matplotlib (Grafikler)
â€¢ Seaborn (Ä°statistiksel grafikler)

API ve Deployment:
â€¢ Flask (REST API)
â€¢ Docker (Containerization)

Hardware:
â€¢ GPU: NVIDIA GeForce RTX 2060 (6 GB VRAM)
â€¢ CUDA 11.8 ile GPU acceleration


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


6. UYGULAMA AKIÅI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AdÄ±m 1: Veri HazÄ±rlama
â€¢ 47,837 ticket verisi yÃ¼klendi
â€¢ Metin temizleme (URL, Ã¶zel karakter)
â€¢ Null deÄŸer kontrolÃ¼
â€¢ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± analizi

AdÄ±m 2: Veri BÃ¶lÃ¼mleme
â€¢ Stratified split (sÄ±nÄ±f dengesi korundu)
â€¢ Train: %80, Validation: %10, Test: %10
â€¢ Random state: 42 (tekrarlanabilirlik)

AdÄ±m 3: Model EÄŸitimleri
â€¢ Baseline: TF-IDF vektÃ¶rleÅŸtirme + LogReg eÄŸitimi
â€¢ LSTM: Word2Vec + tokenization + LSTM eÄŸitimi
â€¢ Ensemble: Model tahminlerini birleÅŸtirme
â€¢ BERT: Pre-trained model + fine-tuning

AdÄ±m 4: DeÄŸerlendirme
â€¢ Test seti ile performans Ã¶lÃ§Ã¼mÃ¼
â€¢ Confusion matrix analizi
â€¢ SÄ±nÄ±f bazÄ±nda metrikler
â€¢ Model karÅŸÄ±laÅŸtÄ±rmasÄ±

AdÄ±m 5: Production Deployment
â€¢ REST API geliÅŸtirildi
â€¢ 3 farklÄ± model endpoint'i
â€¢ Health check servisi


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


7. TEKNÄ°K DETAYLAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Baseline Model (TF-IDF + Logistic Regression):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ VektÃ¶rleÅŸtirme: TF-IDF (ngram: 1-2)
â€¢ SÄ±nÄ±flandÄ±rÄ±cÄ±: Logistic Regression
â€¢ Class weights: Balanced (dengesiz veri Ã§Ã¶zÃ¼mÃ¼)
â€¢ EÄŸitim sÃ¼resi: 5 dakika
â€¢ Model boyutu: 7.5 MB


LSTM Model (Word2Vec + Bidirectional LSTM):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Embedding: Word2Vec (200 dim, skip-gram)
â€¢ Vocabulary: 11,661 kelime
â€¢ Max sequence length: 80 token
â€¢ LSTM units: 128 (Bidirectional)
â€¢ Dropout: 0.3
â€¢ Activation: Softmax
â€¢ Loss: Sparse categorical crossentropy
â€¢ Optimizer: Adam
â€¢ EÄŸitim sÃ¼resi: 30 dakika
â€¢ Model boyutu: 13.4 MB


Ensemble Model:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ YÃ¶ntem: Weighted average
â€¢ Weights: 0.5 (Baseline) + 0.5 (LSTM)
â€¢ 4 farklÄ± kombinasyon test edildi
â€¢ En iyi: Equal weights
â€¢ Tahmin sÃ¼resi: 0.06 saniye


BERT Model (Transfer Learning):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Base model: bert-base-uncased (110M parameters)
â€¢ Fine-tuning: 3 epochs
â€¢ Batch size: 16
â€¢ Max length: 128 tokens
â€¢ Learning rate: 2e-5
â€¢ Optimizer: AdamW
â€¢ Scheduler: Linear warmup
â€¢ Hardware: GPU (CUDA acceleration)
â€¢ EÄŸitim sÃ¼resi: 2.5 saat
â€¢ Model boyutu: 440 MB


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


8. PERFORMANS ANALÄ°ZÄ°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model Accuracy KarÅŸÄ±laÅŸtÄ±rmasÄ±:

Baseline (86.04%)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
LSTM (87.00%)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Ensemble (88.40%)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
BERT (88.82%)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ ğŸ†


Ä°yileÅŸtirme GrafiÄŸi:

BaÅŸlangÄ±Ã§ â†’ Baseline:  86.04%
Baseline â†’ LSTM:       +0.96% iyileÅŸtirme
LSTM â†’ Ensemble:       +1.40% iyileÅŸtirme
Ensemble â†’ BERT:       +0.42% iyileÅŸtirme
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOPLAM Ä°YÄ°LEÅTÄ°RME:    +2.78% (86.04 â†’ 88.82)


SÄ±nÄ±f BazÄ±nda F1-Score (BERT):

Purchase              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.938
Storage               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.922
Access                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   0.915
HR Support            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    0.900
Internal Project      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    0.887
Hardware              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     0.877
Miscellaneous         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     0.847
Admin Rights          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      0.840


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


9. KARÅILAÅTIRMALI ANALÄ°Z
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Neden 4 FarklÄ± Model?

1. Bilimsel YaklaÅŸÄ±m:
   FarklÄ± metodolojilerin karÅŸÄ±laÅŸtÄ±rmalÄ± analizi

2. Avantaj-Dezavantaj Analizi:
   Her modelin gÃ¼Ã§lÃ¼/zayÄ±f yÃ¶nlerini belirleme

3. Optimal Ã‡Ã¶zÃ¼m SeÃ§imi:
   Ä°htiyaca gÃ¶re en uygun modeli belirleme


Hangi Model Ne Zaman KullanÄ±lmalÄ±?

Baseline:
â†’ HÄ±zlÄ± prototip gerekiyorsa
â†’ Hesaplama kaynaÄŸÄ± kÄ±sÄ±tlÄ±ysa
â†’ AÃ§Ä±klanabilir model isteniyorsa

LSTM:
â†’ BaÄŸlam Ã¶nemliyse
â†’ Orta dÃ¼zey doÄŸruluk yeterliyse
â†’ GPU mevcut ama sÄ±nÄ±rlÄ±ysa

Ensemble:
â†’ En yÃ¼ksek gÃ¼venilirlik isteniyorsa
â†’ Kritik kararlar alÄ±nÄ±yorsa
â†’ Tahmin sÃ¼resi Ã§ok Ã¶nemli deÄŸilse

BERT:
â†’ En yÃ¼ksek doÄŸruluk ÅŸartsa
â†’ GPU mevcut ve gÃ¼Ã§lÃ¼yse
â†’ Model boyutu sorun deÄŸilse
â†’ Production ortamÄ±nda yÃ¼ksek kaynak varsa


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


10. ZORLUKLAR VE Ã‡Ã–ZÃœMLER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Zorluk 1: Dengesiz SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: BazÄ± kategorilerde Ã§ok veri var (Hardware: 13,617), 
         bazÄ±larÄ±nda az (Admin Rights: 1,760)

Ã‡Ã¶zÃ¼m:
â€¢ Class weights kullanÄ±mÄ±
â€¢ Stratified split ile dengeli bÃ¶lÃ¼mleme
â€¢ BERT'in robust yapÄ±sÄ± sayesinde etki azaldÄ±


Zorluk 2: GPU Kaynak YÃ¶netimi
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: BERT eÄŸitimi yÃ¼ksek GPU memory gerektiriyor

Ã‡Ã¶zÃ¼m:
â€¢ Batch size optimizasyonu (16)
â€¢ Gradient accumulation
â€¢ Mixed precision training (opsiyonel)


Zorluk 3: Overfitting Riski
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: Modellerin eÄŸitim verisini ezberlemesi

Ã‡Ã¶zÃ¼m:
â€¢ Dropout layers (0.3)
â€¢ Early stopping (patience: 3)
â€¢ Validation set ile sÃ¼rekli kontrol
â€¢ Regularization techniques


Zorluk 4: Model Deployment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problem: 4 farklÄ± modeli production'da sunmak

Ã‡Ã¶zÃ¼m:
â€¢ Flask REST API geliÅŸtirildi
â€¢ Her model iÃ§in ayrÄ± endpoint
â€¢ Docker containerization
â€¢ Model versioning


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


11. ELDE EDÄ°LEN Ã‡IKTILAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EÄŸitilmiÅŸ Modeller:
â€¢ baseline_tfidf_logreg.pkl (7.5 MB)
â€¢ word2vec_lstm_model.h5 (13.4 MB)
â€¢ word2vec_model.bin (14.7 MB)
â€¢ bert_model.pt (440 MB)
â€¢ Tokenizer ve encoder dosyalarÄ±

Analiz RaporlarÄ±:
â€¢ Model comparison reports
â€¢ Confusion matrix grafikleri
â€¢ Per-class performance tables
â€¢ Training history grafikleri

API Servisi:
â€¢ REST API (Flask)
â€¢ 3 farklÄ± model endpoint
â€¢ Health check endpoint
â€¢ JSON response format

DokÃ¼mantasyon:
â€¢ Teknik dokÃ¼mantasyon
â€¢ Kurulum rehberi
â€¢ API kullanÄ±m kÄ±lavuzu
â€¢ Model karÅŸÄ±laÅŸtÄ±rma raporlarÄ±


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


12. SONUÃ‡ VE DEÄERLENDÄ°RME
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proje BaÅŸarÄ±larÄ±:
âœ“ 4 farklÄ± makine Ã¶ÄŸrenmesi yaklaÅŸÄ±mÄ± uygulandÄ±
âœ“ %88.82 test accuracy elde edildi (hedef: %85+)
âœ“ GPU ile optimize edilmiÅŸ eÄŸitim
âœ“ Production-ready sistem geliÅŸtirildi
âœ“ KarÅŸÄ±laÅŸtÄ±rmalÄ± model analizi yapÄ±ldÄ±

Ã–ÄŸrenilenler:
â€¢ Klasik ML vs Derin Ã–ÄŸrenme farklarÄ±
â€¢ Transfer learning'in gÃ¼cÃ¼
â€¢ Ensemble yÃ¶ntemlerinin faydasÄ±
â€¢ GPU programlamanÄ±n Ã¶nemi
â€¢ Production deployment sÃ¼reÃ§leri

Gelecek Ä°yileÅŸtirmeler:
â€¢ Data augmentation ile %90+ accuracy hedefi
â€¢ Cross-validation ile daha gÃ¼venilir metrikler
â€¢ Hyperparameter tuning ile optimizasyon
â€¢ Real-time monitoring ve A/B testing
â€¢ Multi-language support (TÃ¼rkÃ§e adaptasyonu)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


13. KAYNAKÃ‡A
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Akademik Referanslar:
â€¢ Devlin et al. (2018): "BERT: Pre-training of Deep Bidirectional 
  Transformers for Language Understanding"
â€¢ Mikolov et al. (2013): "Efficient Estimation of Word 
  Representations in Vector Space" (Word2Vec)
â€¢ Hochreiter & Schmidhuber (1997): "Long Short-Term Memory"

Teknik Kaynaklar:
â€¢ Hugging Face Transformers Documentation
â€¢ TensorFlow/Keras Official Documentation
â€¢ Scikit-learn User Guide
â€¢ PyTorch Tutorials

Veri Seti:
â€¢ IT Help Desk Ticket Dataset
â€¢ 47,837 labeled support tickets
â€¢ 8 categories, balanced distribution


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


TEÅEKKÃœRLER!

SorularÄ±nÄ±z iÃ§in hazÄ±rÄ±m.

